import time
from data_loader import stream_json_chunks
from cluster_creator import generate_clusters
from export_excel import export_to_excel
from config import DATA_PATH
import pandas as pd

def main():
    print("\U0001F4CA Generating Micro-Clusters and Time Slot Insights...\n")
    total_rows = 1320000
    chunk_size = 500000
    dfs = []
    start_time = time.time()
    load_start = time.time()

    try:
        for i, df_chunk in enumerate(stream_json_chunks(DATA_PATH, chunk_size=chunk_size, total_rows=total_rows)):
            print(f"‚úÖ Loaded chunk {i+1} with {len(df_chunk):,} rows")
            dfs.append(df_chunk)
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Interrupted! Proceeding with loaded data so far...")

    if not dfs:
        print("‚ùå No data loaded. Exiting.")
        return

    df = pd.concat(dfs, ignore_index=True)
    print(f"\n‚ÑπÔ∏è Total rows loaded: {len(df):,}")
    print(f"‚è±Ô∏è Loading Time: {round(time.time() - load_start, 2)} seconds")

    cluster_start = time.time()
    print(f"\nüîç Clustering in progress...")
    clusters = generate_clusters(df)
    print(f"‚è±Ô∏è Clustering Time: {round(time.time() - cluster_start, 2)} seconds")

    export_start = time.time()
    export_to_excel(clusters)
    print(f"‚è±Ô∏è Export Time: {round(time.time() - export_start, 2)} seconds")

    print(f"\n‚úÖ Clustering Complete. Output saved to './output_clusters.xlsx'")
    print(f"\n‚è±Ô∏è Total Runtime: {round(time.time() - start_time, 2)} seconds")

if __name__ == "__main__":
    main()
